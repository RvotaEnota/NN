{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5eb1ea7",
   "metadata": {},
   "source": [
    "# Обучение продвинутой RNN для предсказания замаскированных слов\n",
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab550ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in .\\.venv\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in .\\.venv\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in .\\.venv\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in .\\.venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in .\\.venv\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in .\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in .\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in .\\.venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in .\\.venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in .\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in .\\.venv\\lib\\site-packages (from datasets) (0.32.2)\n",
      "Requirement already satisfied: packaging in .\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in .\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in .\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in .\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in .\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in .\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in .\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in .\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in .\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in .\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in .\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in .\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in .\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import random\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Очистка текста\n",
    "def clean_string(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ba056b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 6650, Val texts: 350\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\") # Загрузили обучающую часть датасета в СЫРОМ виде\n",
    "\n",
    "seq_len = 7 # Зададим длину окна для контекста (тут будет 3 токена до маски, маска и 3 токена после)\n",
    "\n",
    "texts = [line for line in dataset[\"text\"] if len(line.split()) >= seq_len] # Оставим строчки гдне не меньше чем 7 слов, иначе не сможем построить контекст (т.к. задали 7 токенов)\n",
    "\n",
    "cleaned_texts = list(map(clean_string, texts)) # Очищаем с помощью нашей функции\n",
    "\n",
    "max_texts_count = 7000 # Используем только 7к строк, чтобы обучение было недолгое + цпу выдержал\n",
    "\n",
    "train_texts, val_texts = train_test_split(cleaned_texts[:max_texts_count], test_size=0.05, random_state=42)\n",
    "\n",
    "print(f\"Train texts: {len(train_texts)}, Val texts: {len(val_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce2850",
   "metadata": {},
   "source": [
    "Загрузил датасет отфильтровали слишком короткие строки, почистили текст, ограничили объём (7000 строк) и разделили его на обучающую и валидационную выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f74b9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedBertDataset(Dataset): # Архитектура датасета\n",
    "    def __init__(self, texts, tokenizer, seq_len=7): # текст, токенизатор (берт), длина окна\n",
    "        self.samples = []\n",
    "        for line in texts:\n",
    "            token_ids = tokenizer.encode(line, add_special_tokens=False, max_length=512, truncation=True) # токенизация, важно что без sep и сls, т.к. задача другая\n",
    "            if len(token_ids) < seq_len: # Пропуск мелких токенов\n",
    "                continue\n",
    "            for i in range(1, len(token_ids) - 1):\n",
    "                context = token_ids[max(0, i - seq_len//2): i] + [tokenizer.mask_token_id] + token_ids[i+1: i+1+seq_len//2] \n",
    "                # самая важная часть, береем seq_len//2 до текущего token_ids[i - 3 : i], вставляем маску, дабавляем seq_len // 2 токенов после token_ids[i+1 : i+4]\n",
    "                if len(context) < seq_len:\n",
    "                    continue\n",
    "                target = token_ids[i] # Хапомниаем замаскированный токен (y) - наш таргет \n",
    "                self.samples.append((context, target))\n",
    "\n",
    "    def __len__(self): # Вернем количество пар X y в датасете \n",
    "        return len(self.samples) \n",
    "\n",
    "    def __getitem__(self, idx): # обращение к датасету по индексу \n",
    "        x, y = self.samples[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547f6249",
   "metadata": {},
   "source": [
    "Создали класс MaskedBertDataset:\n",
    "- получает строки,\n",
    "- превращает их в токены,\n",
    "- выбирает каждый токен как потенциальный \"пропущенный\",\n",
    "- формирует контекст вокруг него,\n",
    "- возвращает (x, y):\n",
    "- x — токены с <MASK> в центре\n",
    "- y — правильный токен, который заменили\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7b08070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 592449\n",
      "Val dataset size: 29617\n",
      "Train loader size: 9258\n",
      "Val loader size: 463\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\") # Используем берт токенизатор, потому что он подходит под задачу и знает как разбивать на маски\n",
    "\n",
    "train_dataset = MaskedBertDataset(train_texts, tokenizer, seq_len=seq_len)\n",
    "val_dataset = MaskedBertDataset(val_texts, tokenizer, seq_len=seq_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # Бьем нашу выборку на батчи по 64, перемешиваем данные перед эпохой, доджим переобучение\n",
    "val_loader = DataLoader(val_dataset, batch_size=64) # Валидационную выборку не мешаем, так стабильнее результаты\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Val dataset size:\", len(val_dataset))\n",
    "print(\"Train loader size:\", len(train_loader))\n",
    "print(\"Val loader size:\", len(val_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf568ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_batch shape: torch.Size([64, 7])\n",
      "y_batch shape: torch.Size([64])\n",
      "\n",
      "Пример токенов x[0]: [2036, 2275, 2000, 103, 2010, 2299, 15717]\n",
      "Таргет для x[0]: 4685\n",
      "\n",
      "Пример декодированного x[0]: also set to his song skies\n",
      "Замаскированный токен: perform\n"
     ]
    }
   ],
   "source": [
    "# Возьмем первый батч и посмотрим как выглядит\n",
    "for x_batch, y_batch in train_loader:\n",
    "    print(\"x_batch shape:\", x_batch.shape)\n",
    "    print(\"y_batch shape:\", y_batch.shape)\n",
    "\n",
    "    print(\"\\nПример токенов x[0]:\", x_batch[0].tolist())\n",
    "    print(\"Таргет для x[0]:\", y_batch[0].item())\n",
    "\n",
    "    decoded = tokenizer.decode(x_batch[0], skip_special_tokens=True)\n",
    "    print(\"\\nПример декодированного x[0]:\", decoded)\n",
    "\n",
    "    true_token = tokenizer.decode([y_batch[0].item()])\n",
    "    print(\"Замаскированный токен:\", true_token)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be3a2d",
   "metadata": {},
   "source": [
    "На этом этапе мы подготовили данные для обучения модели, которая будет предсказывать замаскированное слово в предложении по контексту вокруг него\n",
    "\n",
    "1. Загрузили датасет WikiText-2, содержащий тексты с Википедии\n",
    "2. Очистили тексты от лишних символов и привели их к нижнему регистру\n",
    "3. Отобрали только те строки, где хотя бы 7 слов (нам нужен контекст: 3 токена до, `<MASK>`, 3 токена после).\n",
    "4. Разбили данные на обучающую и валидационную выборки.\n",
    "5. Реализовали класс `MaskedBertDataset`, который:\n",
    "   - Токенизирует предложение.\n",
    "   - Маскирует по одному токену в каждом окне.\n",
    "   - Сохраняет пары: вход (контекст с `<MASK>`) и правильный токен\n",
    "6. Создали `DataLoader` для обучения и валидации\n",
    "\n",
    "- Обучающих примеров: **592449**\n",
    "- Валидационных примеров: **29617**\n",
    "- Батчей в обучении: **9258**\n",
    "- Батчей в валидации: **463**\n",
    "\n",
    "Теперь у нас есть полноценная выборка для обучения языковой модели, способной угадывать слова по контексту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "803ce82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Type | Combine |     Params\n",
      "-----------------------------------\n",
      "RNN      | sum    |  7,910,202\n",
      "RNN      | concat | 11,817,018\n",
      "GRU      | sum    |  8,042,298\n",
      "GRU      | concat | 11,949,114\n",
      "LSTM     | sum    |  8,108,346\n",
      "LSTM     | concat | 12,015,162\n"
     ]
    }
   ],
   "source": [
    "class BiRNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim=128, rnn_type=\"GRU\", combine=\"concat\"):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_dim)  # токены -> векторы\n",
    "        self.combine = combine  # Запоминаем способ объединения\n",
    "\n",
    "        rnn_cls = {\"RNN\": nn.RNN, \"GRU\": nn.GRU, \"LSTM\": nn.LSTM}[rnn_type]  # Выбираем тип RNN-блока\n",
    "        self.rnn = rnn_cls(hidden_dim, hidden_dim, batch_first=True, bidirectional=True)  # Двунаправленный RNN/GRU/LSTM\n",
    "\n",
    "        out_dim = hidden_dim * 2 if combine == \"concat\" else hidden_dim  # Если concat размер удвоится\n",
    "        self.fc = nn.Linear(out_dim, vocab_size)  # Линейный слой для классификации по словарю\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)  # [batch, seq_len] -> [batch, seq_len, hidden_dim]\n",
    "        out, _ = self.rnn(emb)   # Прогоняем через рекуррентный блок, получаем [batch, seq_len, hidden_dim*2]\n",
    "\n",
    "        center = x.size(1) // 2 \n",
    "        hidden_forward = out[:, center, :out.size(2)//2]   # Скрытое состояние из прямого прохода\n",
    "        hidden_backward = out[:, center, out.size(2)//2:]  # Скрытое состояние из обратного прохода\n",
    "\n",
    "        # либо суммируем, либо конкатенируем\n",
    "        hidden_agg = hidden_forward + hidden_backward if self.combine == \"sum\" else torch.cat([hidden_forward, hidden_backward], dim=1)\n",
    "\n",
    "        linear_out = self.fc(hidden_agg)  # Пропускаем через линейный слой, получаем логиты [batch, vocab_size]\n",
    "        return linear_out\n",
    "    \n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())  # Считаем количество параметров модели\n",
    "\n",
    "\n",
    "vocab_size = tokenizer.vocab_size  # Размер словаря из токенизатора\n",
    "hidden_dim = 128  # Размер скрытого состояния / эмбеддинга\n",
    "\n",
    "rnn_types = [\"RNN\", \"GRU\", \"LSTM\"]  # Варианты рекуррентных блоков\n",
    "combine_methods = [\"sum\", \"concat\"]  # Варианты объединения направлений\n",
    "\n",
    "# Выводим табличку для разных комбинаций\n",
    "print(f\"{'RNN Type':<8} | {'Combine':<6} | {'Params':>10}\")\n",
    "print(\"-\" * 35)\n",
    "for rnn_type in rnn_types:\n",
    "    for combine in combine_methods:\n",
    "        model = BiRNNClassifier(vocab_size, hidden_dim, rnn_type, combine)  # Создаем модель\n",
    "        param_count = count_parameters(model)  # Считаем параметры\n",
    "        print(f\"{rnn_type:<8} | {combine:<6} | {param_count:>10,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00d22d",
   "metadata": {},
   "source": [
    "После выполнения кода мы получили таблицу с количеством параметров модели в зависимости от выбора рекуррентного блока (RNN / GRU / LSTM) \n",
    "и метода объединения скрытых состояний (sum / concat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
