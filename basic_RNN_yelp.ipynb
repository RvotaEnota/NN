{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f11f0c6e",
   "metadata": {},
   "source": [
    "# Реализация простейшей RNN для классификации текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb7dca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\first_sprint\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d152c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета:\n",
      "6500\n",
      "Первый отзыв из датасета:\n",
      "Worst sandwich on Earth.\\nI'd rather eat a dead whore.\\nPlease...never come here.\n",
      "И его рейтинг:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# загрузка датасета\n",
    "raw = pd.read_csv('yelp_reviews.csv')\n",
    "\n",
    "\n",
    "texts = raw['text']\n",
    "labels = raw['label']\n",
    "\n",
    "\n",
    "print('Размер датасета:')\n",
    "print(len(texts))\n",
    "\n",
    "\n",
    "print('Первый отзыв из датасета:')\n",
    "print(texts[0])\n",
    "\n",
    "\n",
    "print('И его рейтинг:')\n",
    "print(labels[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fadc651",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts,\n",
    "    labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cb1d734",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_texts_tokenized = tokenizer(train_texts.tolist(), truncation=True)['input_ids']\n",
    "val_texts_tokenized = tokenizer(val_texts.tolist(), truncation=True)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2c805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём класс кастомного, наследуясь от класса Dataset из PyTorch\n",
    "\n",
    "class YelpDataset(Dataset):\n",
    "    # в конструкторе просто сохраняем тексты и классы\n",
    "    def __init__(self, texts, labels, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "    # возвращаем размер датасета (кол-во текстов)\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # возвращаем текст и его класс\n",
    "        # для текста ограничиваем длину\n",
    "        # не делаем никаких доп. преобразований как padding и masking\n",
    "        return {\n",
    "            'text': torch.tensor(self.texts[idx][:self.max_len], dtype=torch.long),\n",
    "            'label': torch.tensor(self.labels.iloc[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592f2627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кастомная функция collate_fn для формирования батчей\n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch] # получите список текстов в батче\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.long) # получите список классов в батче\n",
    "    lengths = torch.tensor([len(text) for text in texts], dtype=torch.long) # посчитайте список длин текстов в батче \n",
    "    padded_texts = pad_sequence(texts, batch_first=True, padding_value=0) # реализуйте паддинг для текстов\n",
    "\n",
    "\n",
    "    return {\n",
    "        'input_ids': padded_texts, \n",
    "        'lengths': lengths, \n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5b9a95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество батчей в train_dataloader: 82\n",
      "Количество батчей в val_dataloader: 21\n",
      "Размерности батчей:\n",
      "input_ids: torch.Size([64, 256])\n",
      "lengths: torch.Size([64])\n",
      "labels: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# создаём датасеты\n",
    "train_dataset = YelpDataset(texts=train_texts_tokenized, labels=train_labels)\n",
    "val_dataset = YelpDataset(texts=val_texts_tokenized, labels=val_labels)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "# создаём даталоадеры\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "print(f'Количество батчей в train_dataloader: {len(train_dataloader)}')\n",
    "print(f'Количество батчей в val_dataloader: {len(val_dataloader)}')\n",
    "\n",
    "\n",
    "print('Размерности батчей:')\n",
    "for batch in train_dataloader:\n",
    "    print('input_ids:', batch['input_ids'].shape)\n",
    "    print('lengths:', batch['lengths'].shape)\n",
    "    print('labels:', batch['labels'].shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49e194ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# класс модели\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) # напишите слой эмбеддинга с входной размерной vocab_size и выходной embedding_dim\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True) # напишите слой RNN\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # линейный слой для получения скоров классификации\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, lengths):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "        embedded, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        ) # \"запакуйте\" тензор embedded, используя pack_padded_sequence\n",
    "        \n",
    "        packed_output, hidden = self.rnn(packed)# посчитайте выход rnn\n",
    "        output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # Используем последнее скрытое состояние для классификации\n",
    "        last_hidden = hidden[-1]\n",
    "        out = self.fc(last_hidden)# посчитайте скоры для классификации по последнему скрытому состоянию (hidden[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "202d1265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём модель, оптимизатор, объявляем функцию потерь\n",
    "# Получаем размер словаря из токенизатора\n",
    "vocab_size = tokenizer.vocab_size  # например, 30522 для BERT\n",
    "\n",
    "# Объявляем модель\n",
    "model = SimpleRNN(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=128,       # размерность эмбеддинга\n",
    "    hidden_size=128,         # размер скрытого состояния RNN\n",
    "    output_size=5            # 5 классов (отзывов)\n",
    ")\n",
    "\n",
    "# Функция потерь\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Оптимизатор\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a69b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:51<00:00,  1.59it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss = 1.6115, Val Loss = 1.5960\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.05      0.09       238\n",
      "           1       0.26      0.14      0.18       301\n",
      "           2       0.23      0.73      0.35       283\n",
      "           3       0.14      0.02      0.03       242\n",
      "           4       0.34      0.28      0.31       236\n",
      "\n",
      "    accuracy                           0.25      1300\n",
      "   macro avg       0.27      0.24      0.19      1300\n",
      "weighted avg       0.27      0.25      0.20      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:48<00:00,  1.68it/s]\n",
      "100%|██████████| 21/21 [00:00<00:00, 22.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss = 1.5487, Val Loss = 1.5921\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.25      0.25       238\n",
      "           1       0.28      0.14      0.18       301\n",
      "           2       0.24      0.54      0.33       283\n",
      "           3       0.28      0.25      0.26       242\n",
      "           4       0.41      0.10      0.16       236\n",
      "\n",
      "    accuracy                           0.26      1300\n",
      "   macro avg       0.29      0.26      0.24      1300\n",
      "weighted avg       0.29      0.26      0.24      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:53<00:00,  1.52it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 18.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss = 1.4839, Val Loss = 1.5724\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.29      0.31       238\n",
      "           1       0.27      0.40      0.32       301\n",
      "           2       0.23      0.24      0.23       283\n",
      "           3       0.21      0.05      0.07       242\n",
      "           4       0.30      0.38      0.34       236\n",
      "\n",
      "    accuracy                           0.28      1300\n",
      "   macro avg       0.27      0.27      0.26      1300\n",
      "weighted avg       0.27      0.28      0.26      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:48<00:00,  1.70it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss = 1.3938, Val Loss = 1.5770\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.20      0.26       238\n",
      "           1       0.29      0.45      0.35       301\n",
      "           2       0.22      0.27      0.24       283\n",
      "           3       0.24      0.09      0.13       242\n",
      "           4       0.31      0.36      0.34       236\n",
      "\n",
      "    accuracy                           0.28      1300\n",
      "   macro avg       0.29      0.27      0.26      1300\n",
      "weighted avg       0.29      0.28      0.27      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:48<00:00,  1.68it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss = 1.2731, Val Loss = 1.6304\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.45      0.40       238\n",
      "           1       0.34      0.31      0.32       301\n",
      "           2       0.21      0.24      0.22       283\n",
      "           3       0.28      0.30      0.29       242\n",
      "           4       0.37      0.24      0.29       236\n",
      "\n",
      "    accuracy                           0.30      1300\n",
      "   macro avg       0.31      0.31      0.31      1300\n",
      "weighted avg       0.31      0.30      0.30      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:51<00:00,  1.59it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss = 1.1899, Val Loss = 1.6971\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.27      0.32       238\n",
      "           1       0.32      0.46      0.37       301\n",
      "           2       0.23      0.22      0.22       283\n",
      "           3       0.31      0.15      0.20       242\n",
      "           4       0.32      0.44      0.37       236\n",
      "\n",
      "    accuracy                           0.31      1300\n",
      "   macro avg       0.31      0.31      0.30      1300\n",
      "weighted avg       0.31      0.31      0.30      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:56<00:00,  1.45it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 13.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss = 1.0706, Val Loss = 1.7542\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.26      0.32       238\n",
      "           1       0.32      0.39      0.35       301\n",
      "           2       0.20      0.22      0.21       283\n",
      "           3       0.26      0.33      0.29       242\n",
      "           4       0.35      0.22      0.27       236\n",
      "\n",
      "    accuracy                           0.29      1300\n",
      "   macro avg       0.31      0.29      0.29      1300\n",
      "weighted avg       0.30      0.29      0.29      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:48<00:00,  1.69it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 18.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss = 0.9462, Val Loss = 1.8750\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.40      0.36       238\n",
      "           1       0.32      0.40      0.35       301\n",
      "           2       0.20      0.17      0.19       283\n",
      "           3       0.29      0.22      0.25       242\n",
      "           4       0.31      0.27      0.29       236\n",
      "\n",
      "    accuracy                           0.29      1300\n",
      "   macro avg       0.29      0.29      0.29      1300\n",
      "weighted avg       0.29      0.29      0.29      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:48<00:00,  1.70it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 19.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Train Loss = 0.8271, Val Loss = 2.0419\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.34      0.34       238\n",
      "           1       0.33      0.21      0.26       301\n",
      "           2       0.19      0.17      0.18       283\n",
      "           3       0.25      0.42      0.31       242\n",
      "           4       0.32      0.28      0.30       236\n",
      "\n",
      "    accuracy                           0.28      1300\n",
      "   macro avg       0.29      0.28      0.28      1300\n",
      "weighted avg       0.29      0.28      0.27      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:55<00:00,  1.47it/s]\n",
      "100%|██████████| 21/21 [00:01<00:00, 14.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss = 0.6995, Val Loss = 2.1736\n",
      "Метрики классификации (валидация):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.39      0.36       238\n",
      "           1       0.32      0.21      0.26       301\n",
      "           2       0.21      0.22      0.21       283\n",
      "           3       0.27      0.40      0.32       242\n",
      "           4       0.32      0.22      0.26       236\n",
      "\n",
      "    accuracy                           0.28      1300\n",
      "   macro avg       0.29      0.29      0.28      1300\n",
      "weighted avg       0.29      0.28      0.28      1300\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10  # число эпох\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # 1. Получаем данные из батча\n",
    "        inputs = batch['input_ids']\n",
    "        lengths = batch['lengths']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # 2. Обнуляем градиенты\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 3. Прямой проход\n",
    "        outputs = model(inputs, lengths)\n",
    "\n",
    "        # 4. Считаем лосс\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # 5. Назад (обратное распространение ошибки) # считаем градиенты \n",
    "        loss.backward()\n",
    "\n",
    "        # 6. Шаг оптимизации\n",
    "        optimizer.step()\n",
    "\n",
    "        # 7. Сохраняем лосс\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # ----------- Оценка на валидации -----------\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch in tqdm(val_dataloader):\n",
    "        inputs = batch['input_ids']\n",
    "        lengths = batch['lengths']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, lengths)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            y_true.extend(labels.tolist())\n",
    "            y_pred.extend(preds.tolist())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "    print(\"Метрики классификации (валидация):\")\n",
    "    print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3a83e",
   "metadata": {},
   "source": [
    "## Резюме по задаче\n",
    "\n",
    "В этой задаче мы реализовали простую рекуррентную нейронную сеть (SimpleRNN) для классификации отзывов из датасета Yelp. Целью было не построение идеальной модели, а отработка всех этапов работы с RNN: от предобработки текста до обучения модели с учётом длины последовательностей.\n",
    "\n",
    "После запуска обучения видно, что начиная примерно с 5-й эпохи модель начинает переобучаться. Метрики на валидации остаются довольно низкими, но это ожидаемо для такой базовой архитектуры без регуляризации и подбора гиперпараметров.\n",
    "\n",
    "Сама модель получилась простой, что и требовалось — как основа для понимания, как устроены RNN и как они применяются в задачах обработки текстов"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
